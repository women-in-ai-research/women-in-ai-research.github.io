[
  {
    "id": "1",
    "title": "Ep.1: The new WiAIR podcast - Trailer",
    "description": "We are starting a new podcast!It's Women in AI Research, or simply WiAIR. Get ready for inspiring stories of leading women in AI research and their groundbreaking work. Learn from leading women in...",
    "longDescription": "We are starting a new podcast!It's Women in AI Research, or simply WiAIR. Get ready for inspiring stories of leading women in AI research and their groundbreaking work. Learn from leading women in AI, hear powerful stories and join the community of AI researchers that value diversity.",
    "publishDate": "March 7, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/99470815/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-6%2F396092002-44100-2-0cba82530e353.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "2",
    "title": "Ep.2: Limits of Transformers, with Nouha Dziri",
    "description": "Nouha Dziri is an AI research scientist at the Allen Institute for AI, ex-Google DeepMind, ex-Microsoft Research.In this episode, we dive deep into the limitations of transformer models with Nouha...",
    "longDescription": "Nouha Dziri is an AI research scientist at the Allen Institute for AI, ex-Google DeepMind, ex-Microsoft Research.In this episode, we dive deep into the limitations of transformer models with Nouha Dziri, a research scientist at Allen Institute for AI. Nouha shares insights from her research on understanding the capabilities and constraints of LLMs. We discuss the challenges in reasoning, factuality, and ethical considerations that come with deploying these powerful AI systems. This conversation explores both technical aspects and broader implications for the future of AI research.ReferencesNoha's google scholar profileThe Generative AI Paradox:\" What It Can Create, It May Not Understand\"The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"Faith and Fate: Limits of Transformers on CompositionalityWiAIR websiteCheck out the podcast website for the teasers, episode schedule, and more informationhttps://women-in-ai-research.github.ioFollow us atYoutubeLinkedInBlueskyX (Twitter)",
    "publishDate": "March 12, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/99748764/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-12%2F396440226-44100-2-f1e2f13ae2bcc.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "3",
    "title": "Ep.3: Bias in AI, with Amanda Cercas Curry",
    "description": "Dr. Amanda Cercas Curryâ  is a researcher at CENTAI Institute, where she is working on applied NLP, fairness and evaluation.In this episode, we explore the critical issue of bias in AI systems. Amanda...",
    "longDescription": "Dr. Amanda Cercas Curryâ  is a researcher at CENTAI Institute, where she is working on applied NLP, fairness and evaluation.In this episode, we explore the critical issue of bias in AI systems. Amanda shares her expertise on identifying, measuring, and mitigating various forms of bias in language models and other AI applications. We discuss the social and ethical implications of biased AI, and how researchers are working to create more fair and inclusive systems. This episode highlights the importance of diverse perspectives in AI development and the ongoing challenges in the field.Referencesâ Let's Chat Ethicsâ  podcast on Spotifyâ How We Analyzed the COMPAS Recidivism Algorithmâ â Impoverished Language Technology: The Lack of (Social) Class in NLPâ â Signs of Social Class: The Experience of Economic Inequality in Everyday Lifeâ â Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attributionâ â How a chatbot encouraged a man who wanted to kill the Queenâ WiAIR websiteCheck out the podcast website for the teasers, episode schedule, and more informationâ â https://women-in-ai-research.github.ioâ â Follow us atâ â Youtubeâ â â â LinkedInâ â â â Blueskyâ â â â X (Twitter)",
    "publishDate": "April 3, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/100784698/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-3%2F397752876-44100-2-e1d15ba470e7b.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "4",
    "title": "Ep.4: Responsible AI for Health, with Aparna Balagopalan",
    "description": "Aparna Balagopalan is a PhD student in the Department of Electrical Engineering and Computer Science (EECS) at the Massachusetts Institute of Technology.In this episode, we present the intersection...",
    "longDescription": "Aparna Balagopalan is a PhD student in the Department of Electrical Engineering and Computer Science (EECS) at the Massachusetts Institute of Technology.In this episode, we present the intersection of AI and healthcare. Aparna shares her research on developing fair, interpretable, and robust models for healthcare applications. We explore the unique challenges of applying AI in medical contexts, including data quality, collaboration with clinicians, and the critical importance of model transparency. The conversation covers both technical innovations and ethical frameworks necessary for responsible AI deployment in healthcare settings.REFERENCES: Aparna's Google Scholar profileMachine learning for healthcare that matters: Reorienting from technical novelty to equitable impactJudging facts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling dataLEMoN: Label Error Detection using Multimodal NeighborsWiAIR website:https://women-in-ai-research.github.ioFollow us at:LinkedInBlueskyX (Twitter)",
    "publishDate": "April 23, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/101673982/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-23%2F398886155-44100-2-b7a428c205ef2.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "5",
    "title": "Ep.5: Robots with Empathy, with Dr. Angelica Lim",
    "description": "Dr. Angelica Lim, Assistant Professor at Simon Fraser University and Director of the SFU Rosie Lab. Can robots have feelings? In this episode, we explore the intersection of robotics, machine...",
    "longDescription": "Dr. Angelica Lim, Assistant Professor at Simon Fraser University and Director of the SFU Rosie Lab. Can robots have feelings? In this episode, we explore the intersection of robotics, machine learning, and developmental psychology, and consider both the technical challenges and philosophical questions surrounding emotional AI. This conversation offers a glimpse into the future of human-robot interaction and the potential for machines to understand and respond to human emotions.REFERENCES:On Designing User-Friendly Robots: Angelica Lim at TEDxKyoto 2012 Systematic Review of Social Robots for Health and Wellbeing: A Personal Healthcare Journey LensTowards Inclusive HRI: Using Sim2Real to Address Underrepresentation in Emotion Expression Recognition (paper, project website, Github)Contextual Emotion Recognition using Large Vision Language Models (paper, project website)EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters (paper, project website)WiAIR website:â™¾ï¸ https://women-in-ai-research.github.ioFOLLOW US:â™¾ï¸ LinkedInâ™¾ï¸ Blueskyâ™¾ï¸ X (Twitter)#AI #SocialRobotics #EmpathyInAI #EthicalAI #HumanCenteredDesign #WiAIR",
    "publishDate": "May 14, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/102627900/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-14%2F400236878-44100-2-a8806e033473e.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "6",
    "title": "Ep.6: Interpretable AI, with Dr. Faiza Khan Khattak",
    "description": "How can we build AI systems that are fair, explainable, and truly responsible? In this episode of the #WiAIR podcast, we sit down with Dr. Faiza Khan Khattak, the CTO of an innovative AI startup,...",
    "longDescription": "How can we build AI systems that are fair, explainable, and truly responsible? In this episode of the #WiAIR podcast, we sit down with Dr. Faiza Khan Khattak, the CTO of an innovative AI startup, with a rich background in both academia and industry. From fairness in machine learning to the realities of ML deployment in healthcare, this conversation is packed with insights, real-world challenges, and powerful reflections.REFERENCES:MLHOps: Machine Learning Health OperationsUsing Chain-of-Thought Prompting for Interpretable Recognition of Social BiasDialectic Preference Bias in Large Language ModelsThe Impact of Unstated Norms in Bias Analysis of Language ModelsCan Machine Unlearning Reduce Social Bias in Language Models?BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language ModelsğŸ‘‰ Whether you're an AI researcher, a developer working on LLMs, or someone passionate about Responsible AI, this episode is for you.ğŸ“Œ Subscribe to hear more inspiring stories and cutting-edge ideas from women leading the future of AI.WiAIR website.Follow us at:â™¾ï¸ LinkedInâ™¾ï¸ Blueskyâ™¾ï¸ X (Twitter)#WomenInAI #WiAIR #ResponsibleAI #FairnessInAI #AIHealthcare #ExplainableAI #LLMs #AIethics #BiasMitigation #MachineUnlearning #InterpretableAI #AIstartup #AIforGood",
    "publishDate": "June 4, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/103633910/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-5-4%2F401539843-44100-2-48fb9ca5e9bcd.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "7",
    "title": "Ep.7: Decentralized AI, with Wanru Zhao",
    "description": "ğŸ” ğ‚ğšğ§ ğ­ğ¡ğ ğŸğ®ğ­ğ®ğ«ğ ğ¨ğŸ ğ€ğˆ ğ›ğ ğğğœğğ§ğ­ğ«ğšğ¥ğ¢ğ³ğğ? ğ‡ğ¨ğ° ğğ¨ ğ°ğ ğ¬ğœğšğ¥ğ ğ›ğğ²ğ¨ğ§ğ ğ¬ğœğšğ¥ğ¢ğ§ğ  ğ¥ğšğ°ğ¬? ğ€ğ§ğ ğ°ğ¡ğšğ­ ğğ¨ğğ¬ ğ¢ğ­ ğ«ğğšğ¥ğ¥ğ² ğ­ğšğ¤ğ ğ­ğ¨...",
    "longDescription": "ğŸ” ğ‚ğšğ§ ğ­ğ¡ğ ğŸğ®ğ­ğ®ğ«ğ ğ¨ğŸ ğ€ğˆ ğ›ğ ğğğœğğ§ğ­ğ«ğšğ¥ğ¢ğ³ğğ? ğ‡ğ¨ğ° ğğ¨ ğ°ğ ğ¬ğœğšğ¥ğ ğ›ğğ²ğ¨ğ§ğ ğ¬ğœğšğ¥ğ¢ğ§ğ  ğ¥ğšğ°ğ¬? ğ€ğ§ğ ğ°ğ¡ğšğ­ ğğ¨ğğ¬ ğ¢ğ­ ğ«ğğšğ¥ğ¥ğ² ğ­ğšğ¤ğ ğ­ğ¨ ğ›ğ®ğ¢ğ¥ğ ğ¢ğ§ğœğ¥ğ®ğ¬ğ¢ğ¯ğ, ğ¦ğ®ğ¥ğ­ğ¢ğ¥ğ¢ğ§ğ ğ®ğšğ¥ ğ‹ğ‹ğŒğ¬?In this episode of the #WiAIRpodcast, Wanru Zhao discusses decentralized and collaborative AI methods, the limitations of scaling laws, fine-tuning strategies, data attribution challenges in LLMs, and multilingual learning in federated settingsâ€”all while reflecting on her experiences across UK and Canadian research ecosystems. She also shares her academic journey, vision for the future of AI, and how she navigates research roadblocks with creativity and curiosity.ğŸ§  Whether you're building #llms, exploring #FederatedLearning, or just passionate about more inclusive and sustainable AI researchâ€”this episode is packed with insights, encouragement, and visionary thinking.ğŸ‘‰ Watch now and be part of the future of AI thatâ€™s collaborative, global, and radically inclusive.ğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR website:https://women-in-ai-research.github.ioFollow us at:LinkedIn: https://www.linkedin.com/company/women-in-ai-researchBluesky: https://bsky.app/profile/wiair.bsky.socialX (Twitter): https://x.com/WiAIR_podcast#FederatedLearning #AIResearch #LLM #WomenInAI #ScalingLaws #DecentralizedAI #MultilingualAI #NeurIPS2024 #ICLR2024 #DataCentricAI #ModelMerging #WiAIR #WiAIRpodcast #TechForInclusion #RepresentationMatters #MachineLearning #NLP #WomenInSTEM #OpenScience #VectorInstitute #UniversityOfCambridge #AWS #MicrosoftResearch",
    "publishDate": "June 25, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/104603311/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-5-25%2F402763070-44100-2-9c7686145e563.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "8",
    "title": "Ep.8: Generalization in AI, with Dr. Dieuwke Hupkes",
    "description": "A must-listen episode with Dr. Dieuwke Hupkes, a research scientist at #Meta AI Research, where we dive into AI generalization, LLM robustness, and model evaluation in large language models.We...",
    "longDescription": "A must-listen episode with Dr. Dieuwke Hupkes, a research scientist at #Meta AI Research, where we dive into AI generalization, LLM robustness, and model evaluation in large language models.We explore how LLMs handle grammar and hierarchy, how they generalize across tasks and languages, and what consistency tells us about AI alignment.We also talk about Dieuwkeâ€™s journey from physics to NLP, the challenges of peer review, and sustaining a career in researchâ€”plus, how pole dancing helps with focus ğŸ’ªREFERENCES:Dieuwke Hupkes - Google Scholar profileA taxonomy and review of generalization research in NLPWhat's in My Big DataGenBench workshop ( Youtube, website)Separating form and meaning: Using self-consistency to quantify task understanding across multiple sensesFrom form(s) to meaning: Probing the semantic depths of language models using multisense consistencyMultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languagesHow much do language models memorize?Chapters00:00 Introduction to Dieuwke Hupkes and Her Journey05:15 Navigating Challenges in Research07:17 The Peer Review Process: Insights and Frustrations16:23 Being a Woman in AI: Representation and Challenges19:57 Balancing Research and Personal Life23:37 Exploring Consistency and Generalization in Language Models33:31 Generalization Across Modalities35:15 Exploring Generalization Taxonomy40:55 Challenges in Evaluating Generalization44:12 Data Contamination and Generalization50:43 Consistency in Language Models57:23 The Intersection of Consistency and Alignment01:01:15 Current Research DirectionsğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR website:â™¾ï¸ https://women-in-ai-research.github.ioFollow us at:â™¾ï¸ LinkedInâ™¾ï¸ Blueskyâ™¾ï¸ X (Twitter)#LLMs #AIgeneralization #LLMrobustness #AIalignment #ModelEvaluation #MetaAIResearch #WiAIR #WiAIRpodcast",
    "publishDate": "July 16, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/105563294/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-6-16%2F403993584-44100-2-e01d2ca8da44a.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "9",
    "title": "Ep.9: LLM Hallucinations and Machine Unlearning, with Dr. Abhilasha Ravichander",
    "description": "In this episode of the Women in AI Research Podcast, hosts Jekaterina Novikova and Malikeh Ehghaghi engage with Abhilasha Ravichander to discuss the complexities of LLM hallucinations, the...",
    "longDescription": "In this episode of the Women in AI Research Podcast, hosts Jekaterina Novikova and Malikeh Ehghaghi engage with Abhilasha Ravichander to discuss the complexities of LLM hallucinations, the development of factuality benchmarks, and the importance of data transparency and machine unlearning in AI. The conversation also delves into personal experiences in academia and the future directions of research in responsible AI.REFERENCES:Abhilasha Ravichander -- Google Scholar profileWildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity QueriesHALoGEN: Fantastic LLM Hallucinations and Where to Find ThemFActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text GenerationWhat's In My Big Data?Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language ModelsRESTOR: Knowledge Recovery in Machine UnlearningModel State Arithmetic for Machine UnlearningğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR websiteFollow us at:LinkedInBlueskyX (Twitter)#LLMHallucinations #FactualityBenchmarks #MachineUnlearning #DataTransparency #ModelMemorization #ResponsibleAI #GenerativeAI #NLPResearch #WomenInAI #AIResearch #WiAIR #wiairpodcast",
    "publishDate": "August 6, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/106500224/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-7-6%2F405192208-44100-2-8e235f365cfbf.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "10",
    "title": "Ep.10: Unlocking LLM Reasoning, with Simeng Sophia Han",
    "description": "How can we go beyond accuracy to truly understand large language models?In this episode of the Women in AI Research podcast, hosts Jekaterina Novikova and Malikeh Ehghaghi sit down with Simeng Sophia...",
    "longDescription": "How can we go beyond accuracy to truly understand large language models?In this episode of the Women in AI Research podcast, hosts Jekaterina Novikova and Malikeh Ehghaghi sit down with Simeng Sophia Han (PhD candidate at #yaleuniversity , Research Scientist Intern at #metaai , ex #google #deepmind , ex #aws ) to explore the future of ğ‹ğ‹ğŒ ğ«ğğšğ¬ğ¨ğ§ğ¢ğ§ğ , ğğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§, ğšğ§ğ ğğ±ğ©ğ¥ğšğ¢ğ§ğšğ›ğ¥ğ ğ€ğˆ.ğŸŒŸ What youâ€™ll learn in this episode:Why evaluating reasoning goes beyond correctnessHow brain teasers uncover hidden strengths and weaknesses of LLMsThe importance of symbolic reasoning for complex problem solvingThe role of mentorship and early research experiences in shaping careersWhy consistency in AI outputs is essential for building trustHow humans combine brute force and intuition â€” and what this means for AIREFERENCES:Simeng Sophia Han - Google Scholar profileCreativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language ModelsHYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for Enhanced LLM ReasoningP-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning ChainsFolio: Natural Language Reasoning with First-Order LogicğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR websiteFollow us at:LinkedInBlueskyX (Twitter)",
    "publishDate": "August 27, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/107405974/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-7-27%2F406356824-44100-2-c5a383219d305.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "11",
    "title": "Ep.11: Open Science and LLMs, with Dr. Valentina Pyatkin",
    "description": "Can open-source large language models really outperform closed ones like Claude 3.5? ğŸ¤”In this episode of the Women in AI Research podcast, Jekaterina Novikova and Malikeh Ehghaghi engage with...",
    "longDescription": "Can open-source large language models really outperform closed ones like Claude 3.5? ğŸ¤”In this episode of the Women in AI Research podcast, Jekaterina Novikova and Malikeh Ehghaghi engage with Valentina Pyatkin, a postdoctoral researcher at the Allen Institute for AI. We dive deep into the future of open science, LLM research, and extending model capabilities.ğŸ”‘ Topics we cover:Why open-source LLMs sometimes beat closed modelsThe value of releasing datasets, recipes, and training infrastructureThe role of open science in accelerating NLP innovationInsights from Valentinaâ€™s award-winning research journeyREFERENCES:â Valentina's Google Scholar profile â â Olmo: Accelerating the science of language modelsâ â Tulu 3: Pushing Frontiers in Open Language Model Post-Trainingâ â open-instructâ â Generalizing Verifiable Instruction Followingâ â RewardBench 2: Advancing Reward Model Evaluationâ ğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â WiAIR websiteâ â LinkedInâ â Blueskyâ â X (Twitter)",
    "publishDate": "September 17, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/108430980/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-8-17%2F407649270-44100-2-ecebe2460d992.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "12",
    "title": "Ep.12: Can We Trust AI Explanations? Dr. Ana MarasoviÄ‡ on AI Trustworthiness, Explainability & Faithfulness",
    "description": "In this conversation, Jekaterina Novikova and Malikeh Ehgaghi interview Ana MarasoviÄ‡, an expert in AI trustworthiness, focusing on the complexities of explainability, the realities of academic...",
    "longDescription": "In this conversation, Jekaterina Novikova and Malikeh Ehgaghi interview Ana MarasoviÄ‡, an expert in AI trustworthiness, focusing on the complexities of explainability, the realities of academic research, and the dynamics of human-AI collaboration. We discuss the importance of intrinsic and extrinsic trust in AI systems, the challenges of evaluating AI performance, and the implications of synthetic evaluations.REFERENCES:On Evaluating Explanation Utility for Human-AI Decision Making in NLPEffective Human-AI Teams via Learned Natural Language Rules and OnboardingChain-of-Thought Unfaithfulness as Disguised AccuracyOn Measuring Faithfulness or Self-consistency of Natural Language ExplanationsWhat Has Been Lost with Synthetic Evaluation?ğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR websiteFollow us at:LinkedInBlueskyX (Twitter)",
    "publishDate": "October 8, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/109412465/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-9-9%2F408922707-44100-2-ca1344d812897.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "13",
    "title": "Ep.13: Why AI Doesnâ€™t Understand Your Culture? Dr. Vered Shwartz on Cultural Bias in LLMs",
    "description": "Are todayâ€™s AI systems truly global â€” or just Western by design? ğŸŒIn this episode of Women in AI Research, Jekaterina Novikova and Malikeh Ehgaghi speak with Dr. Vered Shwartz (Assistant Professor...",
    "longDescription": "Are todayâ€™s AI systems truly global â€” or just Western by design? ğŸŒIn this episode of Women in AI Research, Jekaterina Novikova and Malikeh Ehgaghi speak with Dr. Vered Shwartz (Assistant Professor at UBC and CIFAR AI Chair at the Vector Institute) about the cultural blind spots in todayâ€™s large language and vision-language models.REFERENCES:Vered Shwartz Google Scholar profileBook \"Lost in Automatic Translation\"Elevator Recognition, by The Scottish Comedy ChannelLocating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on WikipediaECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge TransferWikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language EditionsIs It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4Towards Measuring the Representation of Subjective Global Opinions in Language ModelsI'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language ModelsFrom Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language ModelsCulturalBench: A Robust, Diverse, and ChallengingCultural Benchmark by Human-AI CulturalTeamingğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â WiAIR websiteâ Follow us at:â LinkedInâ â Blueskyâ â X (Twitter)",
    "publishDate": "October 29, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/110366506/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-9-29%2F410171168-44100-2-e24b2b25f2b41.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "14",
    "title": "Ep.14: Multilingual AI, with Dr. Annie En-Shiun Lee",
    "description": "Is English just one of the languages you speak? If so, the AI tools you use might miss things that makes your voice multilingual.In this episode of Women in AI Research, Jekaterina Novikova speaks...",
    "longDescription": "Is English just one of the languages you speak? If so, the AI tools you use might miss things that makes your voice multilingual.In this episode of Women in AI Research, Jekaterina Novikova speaks with Dr. Annie En-Shiun Lee about her work on multilingual and multicultural AI â€” from the widening language gap and the lack of benchmarks for underrepresented languages, to why domain-specific data matters more than just scaling up models. We talk about the limits of cross-lingual transfer, the risks of English-centric reasoning in AI, and the technical, ethical, and cultural challenges of building models that truly serve global communities.References:SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialectsURIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge BasemR3: Multilingual Rubric-Agnostic Reward Reasoning ModelsProxyLM: Predicting language model performance on multilingual tasks via proxy modelsATAIGI: An AI-Powered Multimodal Learning App Leveraging Generative Models for Low-Resource Taiwanese HokkienEnhancing Taiwanese Hokkien Dual Translation by Exploring and Standardizing of Four Writing SystemsAlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse LanguagesIrokobench: A new benchmark for african languages in the age of large language modelsğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â â WiAIR websiteâ â Follow us at:â â LinkedInâ â â â Blueskyâ â â â X (Twitter)",
    "publishDate": "November 19, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/111390548/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-10-19%2F412750621-44100-2-e17e5313ee8b2.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "15",
    "title": "Ep.15: How Does AI Reflect Society, with Dr. Maria Antoniak",
    "description": "AI doesnâ€™t just process text â€” it takes in our cultures, reflects our hierarchies, and can make existing power structures even stronger. In this episode of Women in AI Research, Jekaterina Novikova...",
    "longDescription": "AI doesnâ€™t just process text â€” it takes in our cultures, reflects our hierarchies, and can make existing power structures even stronger. In this episode of Women in AI Research, Jekaterina Novikova and Malikeh Ehgaghi speak with Dr. Maria Antoniak (Assistant Professor at the University of Colorado Boulder) about inclusivity in AI, the dynamics of cultural representation, what trust in AI really means, why LLMs tend to homogenize research cultures, and what maternal healthcare reveals about the deepest ethical challenges in this field.REFERENCES:â Trust No Botâ â A Large-Scale Analysis of Public-Facing, Community-Built Chatbots on Character.AIâ â LLMs as Rsearch Toolsâ â Culture is Not Triviaâ â Research Borderlandsâ â NLP for Maternal Healthcareâ â Data Feminismâ â Epistemic Diversity and Knowledge Collapse in Large Language Modelsâ â Empire of AIâ ğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â â â â WiAIR websiteâ â â â Follow us at:â â â â LinkedInâ â â â â â â â Blueskyâ â â â â â â â X (Twitter)â ",
    "publishDate": "December 10, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/112405330/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-11-10%2F414093922-44100-2-048a6c2a9d9ab.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "16",
    "title": "Ep.16: Do LLMs Understand Meaning? Neuroscience, Evaluation, and the Future of AI, with Maria Ryskina",
    "description": "Do large language models actually understand meaning â€” or are we over-interpreting impressive behavior?In this episode, we speak with Maria Ryskina, CIFAR AI Safety Postdoctoral Fellow at the Vector...",
    "longDescription": "Do large language models actually understand meaning â€” or are we over-interpreting impressive behavior?In this episode, we speak with Maria Ryskina, CIFAR AI Safety Postdoctoral Fellow at the Vector Institute for AI, whose research bridges neuroscience, cognitive science, and artificial intelligence. Together, we unpack what the brain can (and cannot) teach us about modern AI systems â€” and why current evaluation paradigms may be missing something fundamental.We explore how language models can predict brain activity in regions linked to visual processing, what this reveals about cross-modal knowledge, and why scale alone may not resolve deeper conceptual gaps in AI. The conversation also tackles the growing importance of interpretability, especially as AI systems become more embedded in high-stakes, real-world contexts.Beyond technical questions, Maria shares why community matters in AI research, particularly for underrepresented groups â€” and how diversity directly shapes the kinds of scientific questions we ask and the systems we ultimately build.REFERENCESGender Shades: Intersectional Accuracy Disparities in Commercial Gender ClassificationStereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image ModelsLanguage models align with brain regions that represent concepts across modalitiesElements of World Knowledge (EWoK): A Cognition-Inspired Framework for Evaluating Basic World Knowledge in Language ModelsPrompting is not a substitute for probability measurements in large language modelsAuxiliary task demands mask the capabilities of smaller language modelsğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.WiAIR websiteFollow us at:LinkedInBlueskyX (Twitter)",
    "publishDate": "December 31, 2025",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/113269266/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-11-30%2F415195944-44100-2-52f3b507c5571.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "17",
    "title": "Ep.17: AI Safety Beyond Benchmarks -- Dr. Swabha Swayamdipta on Evaluation, Personalization, and Control",
    "description": "As language models become more capable, the hardest questions are no longer just about performance, but about safety, interpretation, and control.In this episode of Women in AI Research, we speak...",
    "longDescription": "As language models become more capable, the hardest questions are no longer just about performance, but about safety, interpretation, and control.In this episode of Women in AI Research, we speak with Swabha Swayamdipta, Assistant Professor of Computer Science at the University of Southern California and co-Associate Director of the USC Center for AI and Society. Swabhaâ€™s research examines how the design and deployment of language models intersect with real-world risks â€” from how models behave in unexpected ways to how seemingly technical choices can have broader societal consequences.We talk about AI safety from multiple angles: what it means when hidden inputs to models can sometimes be inferred from their outputs, why personalization introduces new trade-offs around privacy and user agency, and how assumptions about model behavior can quietly shape downstream harms. Rather than focusing only on accuracy or benchmarks, the conversation asks what kinds of evidence we actually need to trust these systems in practice.REFERENCESBetter Language Model Inversion by Compactly Representing Next-Token DistributionsImproving Language Model Personas via Rationalization with Psychological ScaffoldsOATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM AssistantsUncovering Intervention Opportunities for Suicide Prevention with Language Model AssistantsğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â WiAIR websiteâ Follow us at:â LinkedInâ â Blueskyâ â X (Twitter)",
    "publishDate": "January 21, 2026",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/114319601/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2026-0-21%2F416530670-44100-2-c7a2ee90b0105.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  },
  {
    "id": "18",
    "title": "Ep.18: Faithfulness and Hallucinations in Reasoning Models, with Dr. Letitia Parcalabescu",
    "description": "Are reasoning models actually reasoning â€” or just producing convincing stories?Our guest in this episode of #WiAIRpodcast is Letitia Parcalabescu, the creator of the @AICoffeeBreak youtube channel....",
    "longDescription": "Are reasoning models actually reasoning â€” or just producing convincing stories?Our guest in this episode of #WiAIRpodcast is Letitia Parcalabescu, the creator of the @AICoffeeBreak youtube channel. Letitia joins Jekaterina Novikova for a deep dive into the topics of faithfulness, self-consistency, hallucinations, and the reliability illusion in LLMs and multimodal reasoning models.We discuss why chain-of-thought explanations may not reflect what the model actually did, why RAG does not automatically fix hallucinations, and how visionâ€“language models often rely far more on text than images. We also explore new approaches for grounding and rejection â€” and why models struggle to say \"I don't know.\"Instead of focusing only on benchmark scores, this conversation asks: What kind of evidence do we need to truly trust reasoning models?REFERENCES:On Measuring Faithfulness or Self-consistency of Natural Language ExplanationsDo Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur ProtocolsAI Coffee Break with Letitiahttps://www.youtube.com/c/AICoffeeBreakhttps://x.com/AICoffeeBreakğŸ§ Subscribe to stay updated on new episodes spotlighting brilliant women shaping the future of AI.â WiAIR websiteâ Follow us at:â LinkedInâ â Blueskyâ â X (Twitter)",
    "publishDate": "February 11, 2026",
    "imageUrl": "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/43007946/43007946-1740789260762-b58903ccdaf59.jpg",
    "imagePosition": "center center",
    "episodeLink": "https://anchor.fm/s/100f18168/podcast/play/115334328/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2026-1-11%2F417875545-44100-2-cf27d465684dd.mp3",
    "youtubeLink": null,
    "spotifyLink": null,
    "visible": true
  }
]
